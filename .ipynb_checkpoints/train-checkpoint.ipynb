{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1028eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Untitled1.ipynb\n",
      "importing Jupyter notebook from PSP.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as f\n",
    "import argparse\n",
    "from spikingjelly.clock_driven import neuron, surrogate, functional\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from MIOU_BIOU import Evaluator\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import importipynb\n",
    "import PSPnet_model \n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "# 定义是否使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00845bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89a6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):#继承了Dataset子类\n",
    "    def __init__(self,input_root,label_root,transform_img=None,transform_label=None):\n",
    "        #分别读取输入/标签图片的路径信息\n",
    "        self.input_root=input_root\n",
    "        self.input_files=os.listdir(input_root)#列出指定路径下的所有文件\n",
    "        \n",
    "        self.label_root=label_root\n",
    "        self.label_files=os.listdir(label_root)\n",
    " \n",
    "        self.transforms_img=transform_img\n",
    "        self.transforms_label=transform_label\n",
    "    def __len__(self):\n",
    "        #获取数据集大小\n",
    "        return len(self.input_files)\n",
    "    def __getitem__(self, index):\n",
    "        #根据索引(id)读取对应的图片\n",
    "        input_img_path=os.path.join(self.input_root,self.input_files[index])\n",
    "        input_img=Image.open(input_img_path).convert('RGB')\n",
    "     \n",
    "        label_img_path=os.path.join(self.label_root,self.label_files[index])\n",
    "        label_img=Image.open(label_img_path)\n",
    "        label_img=self.transforms_label(label_img)\n",
    "        \n",
    "        label_img =np.array(label_img)\n",
    "        label_img = torch.FloatTensor(label_img)\n",
    "        input_img=self.transforms_img(input_img)\n",
    "        #print(label_img[0])\n",
    "        return (input_img,label_img)#返"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6ee154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch):\n",
    "    print('\\nEpoch: %d' % (epoch + 1))\n",
    "    net.train()\n",
    "    IOU = []\n",
    "    B_IOU = []\n",
    "    sum_loss = 0.0\n",
    "    total = 0.0\n",
    "    Evaluator_trian =Evaluator(2)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # 准备数据\n",
    "        length = len(trainloader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs= net(inputs)\n",
    "\n",
    "        labels = torch.squeeze(labels)\n",
    "        labels =labels.long()\n",
    "        outputs  = torch.sigmoid(outputs)\n",
    "      \n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        outputs =torch.argmax(outputs,axis=1)\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        outputs = outputs.data.cpu().numpy()\n",
    "    \n",
    "        Evaluator_trian.add_batch(labels,outputs)\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        print('[epoch:%d, iter:%d] Loss: %.03f | '\n",
    "              % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1)))\n",
    "        \n",
    "    iou = Evaluator_trian.Mean_Intersection_over_Union()\n",
    "    b_iou = Evaluator_trian.boundary_iouget()\n",
    "    print(\"训练集miou:\", iou)\n",
    "    print(\"训练集Boundary IoU:\", b_iou)   \n",
    "    writer.add_scalar('训练集损失', sum_loss / (i + 1),epoch+1) #可视化变量loss的值\n",
    "    writer.add_scalar('训练集MIOU', iou, epoch+1)#可视化变量acc的值    \n",
    "    writer.add_scalar('训练集BIOU', b_iou, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1263d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mdoel(epoch):\n",
    "    print(\"Waiting Test!\")\n",
    "    IOU = []\n",
    "    Evaluator_test=Evaluator(2)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            net.eval()\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            #outputs,aux = net(images)\n",
    "            outputs = net(images)\n",
    "            outputs  = torch.sigmoid(outputs)\n",
    "            outputs = outputs.data.cpu().numpy()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            outputs = np.argmax(outputs,axis=1)\n",
    "            Evaluator_test.add_batch(labels,outputs)\n",
    "              \n",
    "        iou = Evaluator_test.Mean_Intersection_over_Union()\n",
    "        b_iou = Evaluator_test.boundary_iouget()\n",
    "        print(\"测试集miou:\", iou)\n",
    "        print(\"测试集Boundary IoU:\", b_iou)\n",
    "        #print('测试IOU：',mean_iou)\n",
    "        writer.add_scalar('测试集MIOU', iou, epoch+1)\n",
    "        writer.add_scalar('测试集BIOU', b_iou, epoch+1)\n",
    "        # 将每次测试结果实时写入acc.txt文件中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28400c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aizir\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "<string>:5: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "Start Training, pspnet\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aizir\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:1, iter:1] Loss: 0.695 | \n",
      "[epoch:1, iter:2] Loss: 0.714 | \n",
      "[epoch:1, iter:3] Loss: 0.711 | \n",
      "[epoch:1, iter:4] Loss: 0.705 | \n",
      "[epoch:1, iter:5] Loss: 0.700 | \n",
      "[epoch:1, iter:6] Loss: 0.691 | \n",
      "[epoch:1, iter:7] Loss: 0.683 | \n",
      "[epoch:1, iter:8] Loss: 0.669 | \n",
      "[epoch:1, iter:9] Loss: 0.664 | \n",
      "[epoch:1, iter:10] Loss: 0.648 | \n",
      "[epoch:1, iter:11] Loss: 0.638 | \n",
      "[epoch:1, iter:12] Loss: 0.630 | \n",
      "[epoch:1, iter:13] Loss: 0.628 | \n",
      "[epoch:1, iter:14] Loss: 0.623 | \n",
      "[epoch:1, iter:15] Loss: 0.614 | \n",
      "[epoch:1, iter:16] Loss: 0.619 | \n",
      "[epoch:1, iter:17] Loss: 0.618 | \n",
      "[epoch:1, iter:18] Loss: 0.613 | \n",
      "[epoch:1, iter:19] Loss: 0.607 | \n",
      "[epoch:1, iter:20] Loss: 0.606 | \n",
      "[epoch:1, iter:21] Loss: 0.603 | \n",
      "[epoch:1, iter:22] Loss: 0.601 | \n",
      "[epoch:1, iter:23] Loss: 0.597 | \n",
      "[epoch:1, iter:24] Loss: 0.595 | \n",
      "[epoch:1, iter:25] Loss: 0.593 | \n",
      "[epoch:1, iter:26] Loss: 0.593 | \n",
      "[epoch:1, iter:27] Loss: 0.592 | \n",
      "[epoch:1, iter:28] Loss: 0.589 | \n",
      "confusion matrix:\n",
      " [[5163118. 1536967.]\n",
      " [ 915515. 1280400.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.5104858848912446\n",
      "训练集Boundary IoU: 0.13358655884205325\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[1132081.       0.]\n",
      " [ 435919.       0.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.3609952168367347\n",
      "测试集Boundary IoU: 0.0\n",
      "0.019510565162951538\n",
      "\n",
      "Epoch: 2\n",
      "[epoch:2, iter:29] Loss: 0.589 | \n",
      "[epoch:2, iter:30] Loss: 0.557 | \n",
      "[epoch:2, iter:31] Loss: 0.545 | \n",
      "[epoch:2, iter:32] Loss: 0.538 | \n",
      "[epoch:2, iter:33] Loss: 0.531 | \n",
      "[epoch:2, iter:34] Loss: 0.530 | \n",
      "[epoch:2, iter:35] Loss: 0.529 | \n",
      "[epoch:2, iter:36] Loss: 0.526 | \n",
      "[epoch:2, iter:37] Loss: 0.534 | \n",
      "[epoch:2, iter:38] Loss: 0.529 | \n",
      "[epoch:2, iter:39] Loss: 0.532 | \n",
      "[epoch:2, iter:40] Loss: 0.531 | \n",
      "[epoch:2, iter:41] Loss: 0.534 | \n",
      "[epoch:2, iter:42] Loss: 0.535 | \n",
      "[epoch:2, iter:43] Loss: 0.535 | \n",
      "[epoch:2, iter:44] Loss: 0.530 | \n",
      "[epoch:2, iter:45] Loss: 0.528 | \n",
      "[epoch:2, iter:46] Loss: 0.526 | \n",
      "[epoch:2, iter:47] Loss: 0.528 | \n",
      "[epoch:2, iter:48] Loss: 0.526 | \n",
      "[epoch:2, iter:49] Loss: 0.528 | \n",
      "[epoch:2, iter:50] Loss: 0.527 | \n",
      "[epoch:2, iter:51] Loss: 0.528 | \n",
      "[epoch:2, iter:52] Loss: 0.526 | \n",
      "[epoch:2, iter:53] Loss: 0.524 | \n",
      "[epoch:2, iter:54] Loss: 0.523 | \n",
      "[epoch:2, iter:55] Loss: 0.521 | \n",
      "[epoch:2, iter:56] Loss: 0.522 | \n",
      "confusion matrix:\n",
      " [[5405150. 1294935.]\n",
      " [ 577587. 1618328.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.6031472750187468\n",
      "训练集Boundary IoU: 0.16838759856965124\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[1078788.   53293.]\n",
      " [ 428738.    7181.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.3529233069397794\n",
      "测试集Boundary IoU: 0.007768149335470514\n",
      "0.018090169943749474\n",
      "\n",
      "Epoch: 3\n",
      "[epoch:3, iter:57] Loss: 0.525 | \n",
      "[epoch:3, iter:58] Loss: 0.518 | \n",
      "[epoch:3, iter:59] Loss: 0.513 | \n",
      "[epoch:3, iter:60] Loss: 0.519 | \n",
      "[epoch:3, iter:61] Loss: 0.511 | \n",
      "[epoch:3, iter:62] Loss: 0.511 | \n",
      "[epoch:3, iter:63] Loss: 0.513 | \n",
      "[epoch:3, iter:64] Loss: 0.507 | \n",
      "[epoch:3, iter:65] Loss: 0.504 | \n",
      "[epoch:3, iter:66] Loss: 0.500 | \n",
      "[epoch:3, iter:67] Loss: 0.506 | \n",
      "[epoch:3, iter:68] Loss: 0.505 | \n",
      "[epoch:3, iter:69] Loss: 0.511 | \n",
      "[epoch:3, iter:70] Loss: 0.510 | \n",
      "[epoch:3, iter:71] Loss: 0.506 | \n",
      "[epoch:3, iter:72] Loss: 0.504 | \n",
      "[epoch:3, iter:73] Loss: 0.505 | \n",
      "[epoch:3, iter:74] Loss: 0.504 | \n",
      "[epoch:3, iter:75] Loss: 0.502 | \n",
      "[epoch:3, iter:76] Loss: 0.501 | \n",
      "[epoch:3, iter:77] Loss: 0.502 | \n",
      "[epoch:3, iter:78] Loss: 0.503 | \n",
      "[epoch:3, iter:79] Loss: 0.501 | \n",
      "[epoch:3, iter:80] Loss: 0.499 | \n",
      "[epoch:3, iter:81] Loss: 0.498 | \n",
      "[epoch:3, iter:82] Loss: 0.496 | \n",
      "[epoch:3, iter:83] Loss: 0.495 | \n",
      "[epoch:3, iter:84] Loss: 0.494 | \n",
      "confusion matrix:\n",
      " [[5454417. 1245668.]\n",
      " [ 446070. 1749845.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.6358539401171113\n",
      "训练集Boundary IoU: 0.17828993860578035\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[970127. 161954.]\n",
      " [107131. 328788.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.6663937387833481\n",
      "测试集Boundary IoU: 0.18741278139940387\n",
      "0.015877852522924733\n",
      "\n",
      "Epoch: 4\n",
      "[epoch:4, iter:85] Loss: 0.449 | \n",
      "[epoch:4, iter:86] Loss: 0.441 | \n",
      "[epoch:4, iter:87] Loss: 0.455 | \n",
      "[epoch:4, iter:88] Loss: 0.457 | \n",
      "[epoch:4, iter:89] Loss: 0.458 | \n",
      "[epoch:4, iter:90] Loss: 0.456 | \n",
      "[epoch:4, iter:91] Loss: 0.457 | \n",
      "[epoch:4, iter:92] Loss: 0.454 | \n",
      "[epoch:4, iter:93] Loss: 0.454 | \n",
      "[epoch:4, iter:94] Loss: 0.455 | \n",
      "[epoch:4, iter:95] Loss: 0.453 | \n",
      "[epoch:4, iter:96] Loss: 0.454 | \n",
      "[epoch:4, iter:97] Loss: 0.453 | \n",
      "[epoch:4, iter:98] Loss: 0.451 | \n",
      "[epoch:4, iter:99] Loss: 0.452 | \n",
      "[epoch:4, iter:100] Loss: 0.451 | \n",
      "[epoch:4, iter:101] Loss: 0.452 | \n",
      "[epoch:4, iter:102] Loss: 0.451 | \n",
      "[epoch:4, iter:103] Loss: 0.450 | \n",
      "[epoch:4, iter:104] Loss: 0.448 | \n",
      "[epoch:4, iter:105] Loss: 0.447 | \n",
      "[epoch:4, iter:106] Loss: 0.446 | \n",
      "[epoch:4, iter:107] Loss: 0.445 | \n",
      "[epoch:4, iter:108] Loss: 0.444 | \n",
      "[epoch:4, iter:109] Loss: 0.443 | \n",
      "[epoch:4, iter:110] Loss: 0.443 | \n",
      "[epoch:4, iter:111] Loss: 0.444 | \n",
      "[epoch:4, iter:112] Loss: 0.443 | \n",
      "confusion matrix:\n",
      " [[5787934.  912151.]\n",
      " [ 301548. 1894367.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.7180775581110979\n",
      "训练集Boundary IoU: 0.19984906834266095\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[999332. 132749.]\n",
      " [108441. 327478.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.6907211517368317\n",
      "测试集Boundary IoU: 0.23153929643107962\n",
      "0.013090169943749475\n",
      "\n",
      "Epoch: 5\n",
      "[epoch:5, iter:113] Loss: 0.448 | \n",
      "[epoch:5, iter:114] Loss: 0.441 | \n",
      "[epoch:5, iter:115] Loss: 0.467 | \n",
      "[epoch:5, iter:116] Loss: 0.466 | \n",
      "[epoch:5, iter:117] Loss: 0.463 | \n",
      "[epoch:5, iter:118] Loss: 0.460 | \n",
      "[epoch:5, iter:119] Loss: 0.461 | \n",
      "[epoch:5, iter:120] Loss: 0.460 | \n",
      "[epoch:5, iter:121] Loss: 0.458 | \n",
      "[epoch:5, iter:122] Loss: 0.456 | \n",
      "[epoch:5, iter:123] Loss: 0.457 | \n",
      "[epoch:5, iter:124] Loss: 0.457 | \n",
      "[epoch:5, iter:125] Loss: 0.455 | \n",
      "[epoch:5, iter:126] Loss: 0.452 | \n",
      "[epoch:5, iter:127] Loss: 0.452 | \n",
      "[epoch:5, iter:128] Loss: 0.451 | \n",
      "[epoch:5, iter:129] Loss: 0.449 | \n",
      "[epoch:5, iter:130] Loss: 0.449 | \n",
      "[epoch:5, iter:131] Loss: 0.450 | \n",
      "[epoch:5, iter:132] Loss: 0.448 | \n",
      "[epoch:5, iter:133] Loss: 0.446 | \n",
      "[epoch:5, iter:134] Loss: 0.446 | \n",
      "[epoch:5, iter:135] Loss: 0.444 | \n",
      "[epoch:5, iter:136] Loss: 0.442 | \n",
      "[epoch:5, iter:137] Loss: 0.442 | \n",
      "[epoch:5, iter:138] Loss: 0.441 | \n",
      "[epoch:5, iter:139] Loss: 0.441 | \n",
      "[epoch:5, iter:140] Loss: 0.440 | \n",
      "confusion matrix:\n",
      " [[5835271.  864814.]\n",
      " [ 307707. 1888208.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.7247988572784816\n",
      "训练集Boundary IoU: 0.22288787079196087\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[1012589.  119492.]\n",
      " [  67693.  368226.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.753481212018036\n",
      "测试集Boundary IoU: 0.25898235583787316\n",
      "0.01\n",
      "\n",
      "Epoch: 6\n",
      "[epoch:6, iter:141] Loss: 0.429 | \n",
      "[epoch:6, iter:142] Loss: 0.439 | \n",
      "[epoch:6, iter:143] Loss: 0.433 | \n",
      "[epoch:6, iter:144] Loss: 0.429 | \n",
      "[epoch:6, iter:145] Loss: 0.426 | \n",
      "[epoch:6, iter:146] Loss: 0.426 | \n",
      "[epoch:6, iter:147] Loss: 0.422 | \n",
      "[epoch:6, iter:148] Loss: 0.422 | \n",
      "[epoch:6, iter:149] Loss: 0.421 | \n",
      "[epoch:6, iter:150] Loss: 0.421 | \n",
      "[epoch:6, iter:151] Loss: 0.421 | \n",
      "[epoch:6, iter:152] Loss: 0.423 | \n",
      "[epoch:6, iter:153] Loss: 0.423 | \n",
      "[epoch:6, iter:154] Loss: 0.422 | \n",
      "[epoch:6, iter:155] Loss: 0.425 | \n",
      "[epoch:6, iter:156] Loss: 0.423 | \n",
      "[epoch:6, iter:157] Loss: 0.423 | \n",
      "[epoch:6, iter:158] Loss: 0.423 | \n",
      "[epoch:6, iter:159] Loss: 0.423 | \n",
      "[epoch:6, iter:160] Loss: 0.423 | \n",
      "[epoch:6, iter:161] Loss: 0.422 | \n",
      "[epoch:6, iter:162] Loss: 0.424 | \n",
      "[epoch:6, iter:163] Loss: 0.425 | \n",
      "[epoch:6, iter:164] Loss: 0.424 | \n",
      "[epoch:6, iter:165] Loss: 0.424 | \n",
      "[epoch:6, iter:166] Loss: 0.424 | \n",
      "[epoch:6, iter:167] Loss: 0.424 | \n",
      "[epoch:6, iter:168] Loss: 0.424 | \n",
      "confusion matrix:\n",
      " [[5941053.  759032.]\n",
      " [ 264431. 1931484.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.7533451402053\n",
      "训练集Boundary IoU: 0.26684162756600865\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[1032989.   99092.]\n",
      " [  71344.  364575.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.7699044054482342\n",
      "测试集Boundary IoU: 0.2885819272788281\n",
      "0.006909830056250526\n",
      "\n",
      "Epoch: 7\n",
      "[epoch:7, iter:169] Loss: 0.396 | \n",
      "[epoch:7, iter:170] Loss: 0.396 | \n",
      "[epoch:7, iter:171] Loss: 0.410 | \n",
      "[epoch:7, iter:172] Loss: 0.414 | \n",
      "[epoch:7, iter:173] Loss: 0.412 | \n",
      "[epoch:7, iter:174] Loss: 0.411 | \n",
      "[epoch:7, iter:175] Loss: 0.411 | \n",
      "[epoch:7, iter:176] Loss: 0.412 | \n",
      "[epoch:7, iter:177] Loss: 0.412 | \n",
      "[epoch:7, iter:178] Loss: 0.414 | \n",
      "[epoch:7, iter:179] Loss: 0.414 | \n",
      "[epoch:7, iter:180] Loss: 0.413 | \n",
      "[epoch:7, iter:181] Loss: 0.414 | \n",
      "[epoch:7, iter:182] Loss: 0.414 | \n",
      "[epoch:7, iter:183] Loss: 0.414 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:7, iter:184] Loss: 0.415 | \n",
      "[epoch:7, iter:185] Loss: 0.414 | \n",
      "[epoch:7, iter:186] Loss: 0.415 | \n",
      "[epoch:7, iter:187] Loss: 0.415 | \n",
      "[epoch:7, iter:188] Loss: 0.416 | \n",
      "[epoch:7, iter:189] Loss: 0.416 | \n",
      "[epoch:7, iter:190] Loss: 0.415 | \n",
      "[epoch:7, iter:191] Loss: 0.417 | \n",
      "[epoch:7, iter:192] Loss: 0.416 | \n",
      "[epoch:7, iter:193] Loss: 0.414 | \n",
      "[epoch:7, iter:194] Loss: 0.414 | \n",
      "[epoch:7, iter:195] Loss: 0.416 | \n",
      "[epoch:7, iter:196] Loss: 0.416 | \n",
      "confusion matrix:\n",
      " [[6013456.  686629.]\n",
      " [ 251189. 1944726.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.7698715130936493\n",
      "训练集Boundary IoU: 0.29545034690991245\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[1048022.   84059.]\n",
      " [  80969.  354950.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.773290588870511\n",
      "测试集Boundary IoU: 0.3158723920509885\n",
      "0.0041221474770752695\n",
      "\n",
      "Epoch: 8\n",
      "[epoch:8, iter:197] Loss: 0.409 | \n",
      "[epoch:8, iter:198] Loss: 0.410 | \n",
      "[epoch:8, iter:199] Loss: 0.408 | \n",
      "[epoch:8, iter:200] Loss: 0.415 | \n",
      "[epoch:8, iter:201] Loss: 0.411 | \n",
      "[epoch:8, iter:202] Loss: 0.409 | \n",
      "[epoch:8, iter:203] Loss: 0.410 | \n",
      "[epoch:8, iter:204] Loss: 0.411 | \n",
      "[epoch:8, iter:205] Loss: 0.413 | \n",
      "[epoch:8, iter:206] Loss: 0.412 | \n",
      "[epoch:8, iter:207] Loss: 0.410 | \n",
      "[epoch:8, iter:208] Loss: 0.411 | \n",
      "[epoch:8, iter:209] Loss: 0.412 | \n",
      "[epoch:8, iter:210] Loss: 0.412 | \n",
      "[epoch:8, iter:211] Loss: 0.412 | \n",
      "[epoch:8, iter:212] Loss: 0.412 | \n",
      "[epoch:8, iter:213] Loss: 0.410 | \n",
      "[epoch:8, iter:214] Loss: 0.408 | \n",
      "[epoch:8, iter:215] Loss: 0.408 | \n",
      "[epoch:8, iter:216] Loss: 0.408 | \n",
      "[epoch:8, iter:217] Loss: 0.408 | \n",
      "[epoch:8, iter:218] Loss: 0.409 | \n",
      "[epoch:8, iter:219] Loss: 0.410 | \n",
      "[epoch:8, iter:220] Loss: 0.410 | \n",
      "[epoch:8, iter:221] Loss: 0.409 | \n",
      "[epoch:8, iter:222] Loss: 0.409 | \n",
      "[epoch:8, iter:223] Loss: 0.409 | \n",
      "[epoch:8, iter:224] Loss: 0.409 | \n",
      "confusion matrix:\n",
      " [[6040641.  659444.]\n",
      " [ 226286. 1969629.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.7809614475547901\n",
      "训练集Boundary IoU: 0.31382076516274754\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[1072099.   59982.]\n",
      " [  97290.  338629.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.777463609388917\n",
      "测试集Boundary IoU: 0.3442840482761687\n",
      "0.0019098300562505265\n",
      "\n",
      "Epoch: 9\n",
      "[epoch:9, iter:225] Loss: 0.395 | \n",
      "[epoch:9, iter:226] Loss: 0.403 | \n",
      "[epoch:9, iter:227] Loss: 0.404 | \n",
      "[epoch:9, iter:228] Loss: 0.405 | \n",
      "[epoch:9, iter:229] Loss: 0.406 | \n",
      "[epoch:9, iter:230] Loss: 0.410 | \n",
      "[epoch:9, iter:231] Loss: 0.407 | \n",
      "[epoch:9, iter:232] Loss: 0.409 | \n",
      "[epoch:9, iter:233] Loss: 0.407 | \n",
      "[epoch:9, iter:234] Loss: 0.407 | \n",
      "[epoch:9, iter:235] Loss: 0.404 | \n",
      "[epoch:9, iter:236] Loss: 0.406 | \n",
      "[epoch:9, iter:237] Loss: 0.406 | \n",
      "[epoch:9, iter:238] Loss: 0.405 | \n",
      "[epoch:9, iter:239] Loss: 0.404 | \n",
      "[epoch:9, iter:240] Loss: 0.403 | \n",
      "[epoch:9, iter:241] Loss: 0.402 | \n",
      "[epoch:9, iter:242] Loss: 0.404 | \n",
      "[epoch:9, iter:243] Loss: 0.404 | \n",
      "[epoch:9, iter:244] Loss: 0.405 | \n",
      "[epoch:9, iter:245] Loss: 0.405 | \n",
      "[epoch:9, iter:246] Loss: 0.405 | \n",
      "[epoch:9, iter:247] Loss: 0.406 | \n",
      "[epoch:9, iter:248] Loss: 0.405 | \n",
      "[epoch:9, iter:249] Loss: 0.404 | \n",
      "[epoch:9, iter:250] Loss: 0.403 | \n",
      "[epoch:9, iter:251] Loss: 0.403 | \n",
      "[epoch:9, iter:252] Loss: 0.404 | \n",
      "confusion matrix:\n",
      " [[6087256.  612829.]\n",
      " [ 216626. 1979289.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.7923839257730063\n",
      "训练集Boundary IoU: 0.33567310526345434\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[1039894.   92187.]\n",
      " [  63274.  372645.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.7877855670734368\n",
      "测试集Boundary IoU: 0.3424373529235302\n",
      "0.0004894348370484647\n",
      "\n",
      "Epoch: 10\n",
      "[epoch:10, iter:253] Loss: 0.401 | \n",
      "[epoch:10, iter:254] Loss: 0.401 | \n",
      "[epoch:10, iter:255] Loss: 0.414 | \n",
      "[epoch:10, iter:256] Loss: 0.408 | \n",
      "[epoch:10, iter:257] Loss: 0.405 | \n",
      "[epoch:10, iter:258] Loss: 0.403 | \n",
      "[epoch:10, iter:259] Loss: 0.405 | \n",
      "[epoch:10, iter:260] Loss: 0.404 | \n",
      "[epoch:10, iter:261] Loss: 0.405 | \n",
      "[epoch:10, iter:262] Loss: 0.405 | \n",
      "[epoch:10, iter:263] Loss: 0.404 | \n",
      "[epoch:10, iter:264] Loss: 0.402 | \n",
      "[epoch:10, iter:265] Loss: 0.402 | \n",
      "[epoch:10, iter:266] Loss: 0.402 | \n",
      "[epoch:10, iter:267] Loss: 0.402 | \n",
      "[epoch:10, iter:268] Loss: 0.403 | \n",
      "[epoch:10, iter:269] Loss: 0.402 | \n",
      "[epoch:10, iter:270] Loss: 0.401 | \n",
      "[epoch:10, iter:271] Loss: 0.402 | \n",
      "[epoch:10, iter:272] Loss: 0.401 | \n",
      "[epoch:10, iter:273] Loss: 0.402 | \n",
      "[epoch:10, iter:274] Loss: 0.402 | \n",
      "[epoch:10, iter:275] Loss: 0.401 | \n",
      "[epoch:10, iter:276] Loss: 0.401 | \n",
      "[epoch:10, iter:277] Loss: 0.401 | \n",
      "[epoch:10, iter:278] Loss: 0.401 | \n",
      "[epoch:10, iter:279] Loss: 0.402 | \n",
      "[epoch:10, iter:280] Loss: 0.402 | \n",
      "confusion matrix:\n",
      " [[6087656.  612429.]\n",
      " [ 209470. 1986445.]]\n",
      "sum 8896000.0\n",
      "训练集miou: 0.7941928517672285\n",
      "训练集Boundary IoU: 0.34491584081720367\n",
      "Waiting Test!\n",
      "confusion matrix:\n",
      " [[1033196.   98885.]\n",
      " [  54959.  380960.]]\n",
      "sum 1568000.0\n",
      "测试集miou: 0.791366343990131\n",
      "测试集Boundary IoU: 0.33781385402998987\n",
      "0.0\n",
      "Training Finished, TotalEPOCH=10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PSPNet(\n",
       "  (pool1): PyramidPool(\n",
       "    (pooldown): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=1)\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pool2): PyramidPool(\n",
       "    (pooldown): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=2)\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pool3): PyramidPool(\n",
       "    (pooldown): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=3)\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pool6): PyramidPool(\n",
       "    (pooldown): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=6)\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_cat): Sequential(\n",
       "    (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (layer_aux): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(1024, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (outpot_layer): Sequential(\n",
       "    (0): Conv2d(4, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.95, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "EPOCH = 10#遍历数据集次数\n",
    "pre_epoch = 0  # 定义已经遍历数据集的次数\n",
    "BATCH_SIZE = 10  #批处理尺寸(batch_size)\n",
    "LR = 0.02     #学习率\n",
    "#log_path='C:/Users/aizir/network_homework/path' #这是存放你要显示的数据的绝对路径\n",
    "#try:\n",
    " #   shutil.rmtree(log_path) #当log文件存在时删除文件夹。记得在代码最开始import shutil\n",
    " #   print(\"The folder has been emptied.\")\n",
    "#except:\n",
    "   # print(\"The folder does not exist.\") #当log文件不存在时，直接打印“文件夹不存在”。\n",
    "\n",
    "writer = SummaryWriter('./hhpath/to/log')\n",
    "\n",
    "input_root='C:\\\\Users\\\\aizir\\\\PSPnet\\\\weizmann_horse_db\\\\horse_train'\n",
    "label_root='C:\\\\Users\\\\aizir\\\\PSPnet\\\\weizmann_horse_db\\\\mask_train'\n",
    "\n",
    "input_test = 'C:\\\\Users\\\\aizir\\\\PSPnet\\\\weizmann_horse_db\\\\horse_test'\n",
    "label_test='C:\\\\Users\\\\aizir\\\\PSPnet\\\\weizmann_horse_db\\\\mask_test'\n",
    "# 准备数据集并预处理\n",
    "\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.Resize((160, 200), interpolation=Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    " \n",
    "])\n",
    "transform_label= transforms.Compose([\n",
    "    transforms.Resize((160, 200), interpolation=Image.NEAREST),\n",
    "\n",
    "])\n",
    "\n",
    "dataset_train= MyDataset(input_root, label_root, transform_img=transform_img,transform_label=transform_label )\n",
    "print(len(dataset_train))\n",
    "trainloader=torch.utils.data.DataLoader(dataset_train,batch_size=10, shuffle=True)\n",
    "dataset_test= MyDataset(input_test , label_test, transform_img=transform_img,transform_label=transform_label)\n",
    "testloader=torch.utils.data.DataLoader(dataset_test,batch_size=10, shuffle=False)\n",
    "\n",
    "# 模型定义-pspnet\n",
    "net =PSPnet_model.PSPNet(classes=2,poolchannl=2048,outpoolchannl=512).to(device)\n",
    "\n",
    "\n",
    "# 定义损失函数和优化方式\n",
    "weights=torch.FloatTensor([1.6,3.7]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = weights)  #损失函数为交叉熵，多用于多分类问题\n",
    "optimizer =optim.Adam(net.parameters(), weight_decay=0.000001, lr=LR)#Adam优化器\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCH)\n",
    "\n",
    "# 训练\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Start Training, pspnet\")  # 定义遍历数据集的次数\n",
    "    for epoch in range(pre_epoch, EPOCH):\n",
    "        train_model(epoch)\n",
    "        # 每训练完一个epoch测试一下准确率\n",
    "        test_mdoel(epoch)\n",
    "        scheduler.step()\n",
    "        print(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    torch.save(net.state_dict(),\"./pspnetepoch_100.pkl\") \n",
    "    print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)\n",
    "                    \n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c189d652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 200])\n",
      "1973445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Image.save of <PIL.Image.Image image mode=I size=200x160 at 0x2433AE59700>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "vis_path='C://Users//aizir//PSPnet//weizmann_horse_db//horse_train/horse001.png'\n",
    "mask_path='C:\\\\Users\\\\aizir\\\\PSPnet\\\\weizmann_horse_db\\\\mask_train\\\\horse001.png'\n",
    "vis=Image.open(vis_path).convert('RGB')\n",
    "vis_img=transform_img(vis)\n",
    "vis_img= vis_img.unsqueeze(dim=0).cuda()\n",
    "img = np.array([[0 for col in range(200)] for row in range(160)])\n",
    "\n",
    "dd=Image.open(mask_path)\n",
    "dd=transform_label(dd)\n",
    "\n",
    "dd =np.array(dd)    \n",
    "dd= torch.FloatTensor(dd)\n",
    "print(dd.shape)\n",
    "#print(dd.shape)\n",
    "for i in range(160):\n",
    "    for j in range(200):\n",
    "        if dd[i][j]==1:\n",
    "            img[i][j] = 255\n",
    "        else:\n",
    "            img[i][j] = 0\n",
    "            \n",
    "image = Image.fromarray(img)\n",
    "image.show() \n",
    "image.save\n",
    "\n",
    "pre =net(vis_img)\n",
    "pre =torch.squeeze(pre)\n",
    "pre  = torch.sigmoid(pre).data.cpu().numpy()\n",
    "img = np.array([[0 for col in range(200)] for row in range(160)])\n",
    "for i in range(160):\n",
    "    for j in range(200):\n",
    "        if pre[0][i][j]<pre[1][i][j]:\n",
    "            img[i][j] =255\n",
    "        else:\n",
    "            img[i][j] =0\n",
    "print(img.sum())\n",
    "image = Image.fromarray(img)\n",
    "image.show() \n",
    "image.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5177af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tensorboard  --logdir=./hhpath/to/log --port 8120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ad3ae",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
