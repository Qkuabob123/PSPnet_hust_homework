{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c30f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from resnet101.ipynb\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "            Conv2d-4         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-5         [-1, 64, 112, 112]             128\n",
      "              ReLU-6         [-1, 64, 112, 112]               0\n",
      "            Conv2d-7        [-1, 128, 112, 112]          73,728\n",
      "       BatchNorm2d-8        [-1, 128, 112, 112]             256\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]           8,192\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "             ReLU-16           [-1, 64, 56, 56]               0\n",
      "           Conv2d-17          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-18          [-1, 256, 56, 56]             512\n",
      "           Conv2d-19          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-20          [-1, 256, 56, 56]             512\n",
      "             ReLU-21          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-22          [-1, 256, 56, 56]               0\n",
      "           Conv2d-23           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
      "             ReLU-25           [-1, 64, 56, 56]               0\n",
      "           Conv2d-26           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-27           [-1, 64, 56, 56]             128\n",
      "             ReLU-28           [-1, 64, 56, 56]               0\n",
      "           Conv2d-29          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-30          [-1, 256, 56, 56]             512\n",
      "             ReLU-31          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-32          [-1, 256, 56, 56]               0\n",
      "           Conv2d-33           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-34           [-1, 64, 56, 56]             128\n",
      "             ReLU-35           [-1, 64, 56, 56]               0\n",
      "           Conv2d-36           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-37           [-1, 64, 56, 56]             128\n",
      "             ReLU-38           [-1, 64, 56, 56]               0\n",
      "           Conv2d-39          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-40          [-1, 256, 56, 56]             512\n",
      "             ReLU-41          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-42          [-1, 256, 56, 56]               0\n",
      "           Conv2d-43          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-44          [-1, 128, 56, 56]             256\n",
      "             ReLU-45          [-1, 128, 56, 56]               0\n",
      "           Conv2d-46          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 28, 28]             256\n",
      "             ReLU-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-51          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-52          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-53          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-54          [-1, 512, 28, 28]               0\n",
      "           Conv2d-55          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 128, 28, 28]             256\n",
      "             ReLU-57          [-1, 128, 28, 28]               0\n",
      "           Conv2d-58          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-59          [-1, 128, 28, 28]             256\n",
      "             ReLU-60          [-1, 128, 28, 28]               0\n",
      "           Conv2d-61          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-62          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-63          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-64          [-1, 512, 28, 28]               0\n",
      "           Conv2d-65          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 128, 28, 28]             256\n",
      "             ReLU-67          [-1, 128, 28, 28]               0\n",
      "           Conv2d-68          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-69          [-1, 128, 28, 28]             256\n",
      "             ReLU-70          [-1, 128, 28, 28]               0\n",
      "           Conv2d-71          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-72          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-73          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-74          [-1, 512, 28, 28]               0\n",
      "           Conv2d-75          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 128, 28, 28]             256\n",
      "             ReLU-77          [-1, 128, 28, 28]               0\n",
      "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
      "             ReLU-80          [-1, 128, 28, 28]               0\n",
      "           Conv2d-81          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-82          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-83          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-84          [-1, 512, 28, 28]               0\n",
      "           Conv2d-85          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-86          [-1, 256, 28, 28]             512\n",
      "             ReLU-87          [-1, 256, 28, 28]               0\n",
      "           Conv2d-88          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-89          [-1, 256, 14, 14]             512\n",
      "             ReLU-90          [-1, 256, 14, 14]               0\n",
      "           Conv2d-91         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-92         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-93         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-94         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-95         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-96         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-97          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-98          [-1, 256, 14, 14]             512\n",
      "             ReLU-99          [-1, 256, 14, 14]               0\n",
      "          Conv2d-100          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-101          [-1, 256, 14, 14]             512\n",
      "            ReLU-102          [-1, 256, 14, 14]               0\n",
      "          Conv2d-103         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-104         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-105         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-106         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-107          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-108          [-1, 256, 14, 14]             512\n",
      "            ReLU-109          [-1, 256, 14, 14]               0\n",
      "          Conv2d-110          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-111          [-1, 256, 14, 14]             512\n",
      "            ReLU-112          [-1, 256, 14, 14]               0\n",
      "          Conv2d-113         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-114         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-115         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-116         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-117          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
      "            ReLU-119          [-1, 256, 14, 14]               0\n",
      "          Conv2d-120          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
      "            ReLU-122          [-1, 256, 14, 14]               0\n",
      "          Conv2d-123         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-124         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-125         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-126         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-127          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-128          [-1, 256, 14, 14]             512\n",
      "            ReLU-129          [-1, 256, 14, 14]               0\n",
      "          Conv2d-130          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-131          [-1, 256, 14, 14]             512\n",
      "            ReLU-132          [-1, 256, 14, 14]               0\n",
      "          Conv2d-133         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-134         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-135         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-136         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-137          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-138          [-1, 256, 14, 14]             512\n",
      "            ReLU-139          [-1, 256, 14, 14]               0\n",
      "          Conv2d-140          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-141          [-1, 256, 14, 14]             512\n",
      "            ReLU-142          [-1, 256, 14, 14]               0\n",
      "          Conv2d-143         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-144         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-145         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-146         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-147          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-148          [-1, 256, 14, 14]             512\n",
      "            ReLU-149          [-1, 256, 14, 14]               0\n",
      "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
      "            ReLU-152          [-1, 256, 14, 14]               0\n",
      "          Conv2d-153         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-154         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-155         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-156         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-157          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-158          [-1, 256, 14, 14]             512\n",
      "            ReLU-159          [-1, 256, 14, 14]               0\n",
      "          Conv2d-160          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-161          [-1, 256, 14, 14]             512\n",
      "            ReLU-162          [-1, 256, 14, 14]               0\n",
      "          Conv2d-163         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-164         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-165         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-166         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-167          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
      "            ReLU-169          [-1, 256, 14, 14]               0\n",
      "          Conv2d-170          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-171          [-1, 256, 14, 14]             512\n",
      "            ReLU-172          [-1, 256, 14, 14]               0\n",
      "          Conv2d-173         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-174         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-175         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-176         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-177          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-178          [-1, 256, 14, 14]             512\n",
      "            ReLU-179          [-1, 256, 14, 14]               0\n",
      "          Conv2d-180          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-181          [-1, 256, 14, 14]             512\n",
      "            ReLU-182          [-1, 256, 14, 14]               0\n",
      "          Conv2d-183         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-184         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-185         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-186         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-187          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-188          [-1, 256, 14, 14]             512\n",
      "            ReLU-189          [-1, 256, 14, 14]               0\n",
      "          Conv2d-190          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
      "            ReLU-192          [-1, 256, 14, 14]               0\n",
      "          Conv2d-193         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-194         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-195         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-196         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-197          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-198          [-1, 256, 14, 14]             512\n",
      "            ReLU-199          [-1, 256, 14, 14]               0\n",
      "          Conv2d-200          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-201          [-1, 256, 14, 14]             512\n",
      "            ReLU-202          [-1, 256, 14, 14]               0\n",
      "          Conv2d-203         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-204         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-205         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-206         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-207          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-208          [-1, 256, 14, 14]             512\n",
      "            ReLU-209          [-1, 256, 14, 14]               0\n",
      "          Conv2d-210          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-211          [-1, 256, 14, 14]             512\n",
      "            ReLU-212          [-1, 256, 14, 14]               0\n",
      "          Conv2d-213         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-214         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-215         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-216         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-217          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-218          [-1, 256, 14, 14]             512\n",
      "            ReLU-219          [-1, 256, 14, 14]               0\n",
      "          Conv2d-220          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-221          [-1, 256, 14, 14]             512\n",
      "            ReLU-222          [-1, 256, 14, 14]               0\n",
      "          Conv2d-223         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-224         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-225         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-226         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-227          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-228          [-1, 256, 14, 14]             512\n",
      "            ReLU-229          [-1, 256, 14, 14]               0\n",
      "          Conv2d-230          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-231          [-1, 256, 14, 14]             512\n",
      "            ReLU-232          [-1, 256, 14, 14]               0\n",
      "          Conv2d-233         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-234         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-235         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-236         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-237          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-238          [-1, 256, 14, 14]             512\n",
      "            ReLU-239          [-1, 256, 14, 14]               0\n",
      "          Conv2d-240          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-241          [-1, 256, 14, 14]             512\n",
      "            ReLU-242          [-1, 256, 14, 14]               0\n",
      "          Conv2d-243         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-244         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-245         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-246         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-247          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-248          [-1, 256, 14, 14]             512\n",
      "            ReLU-249          [-1, 256, 14, 14]               0\n",
      "          Conv2d-250          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-251          [-1, 256, 14, 14]             512\n",
      "            ReLU-252          [-1, 256, 14, 14]               0\n",
      "          Conv2d-253         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-254         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-255         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-256         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-257          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-258          [-1, 256, 14, 14]             512\n",
      "            ReLU-259          [-1, 256, 14, 14]               0\n",
      "          Conv2d-260          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-261          [-1, 256, 14, 14]             512\n",
      "            ReLU-262          [-1, 256, 14, 14]               0\n",
      "          Conv2d-263         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-264         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-265         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-266         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-267          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-268          [-1, 256, 14, 14]             512\n",
      "            ReLU-269          [-1, 256, 14, 14]               0\n",
      "          Conv2d-270          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-271          [-1, 256, 14, 14]             512\n",
      "            ReLU-272          [-1, 256, 14, 14]               0\n",
      "          Conv2d-273         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-274         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-275         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-276         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-277          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-278          [-1, 256, 14, 14]             512\n",
      "            ReLU-279          [-1, 256, 14, 14]               0\n",
      "          Conv2d-280          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-281          [-1, 256, 14, 14]             512\n",
      "            ReLU-282          [-1, 256, 14, 14]               0\n",
      "          Conv2d-283         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-284         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-285         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-286         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-287          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-288          [-1, 256, 14, 14]             512\n",
      "            ReLU-289          [-1, 256, 14, 14]               0\n",
      "          Conv2d-290          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-291          [-1, 256, 14, 14]             512\n",
      "            ReLU-292          [-1, 256, 14, 14]               0\n",
      "          Conv2d-293         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-294         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-295         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-296         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-297          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-298          [-1, 256, 14, 14]             512\n",
      "            ReLU-299          [-1, 256, 14, 14]               0\n",
      "          Conv2d-300          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-301          [-1, 256, 14, 14]             512\n",
      "            ReLU-302          [-1, 256, 14, 14]               0\n",
      "          Conv2d-303         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-304         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-305         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-306         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-307          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-308          [-1, 256, 14, 14]             512\n",
      "            ReLU-309          [-1, 256, 14, 14]               0\n",
      "          Conv2d-310          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-311          [-1, 256, 14, 14]             512\n",
      "            ReLU-312          [-1, 256, 14, 14]               0\n",
      "          Conv2d-313         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-314         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-315         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-316         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-317          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-318          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-319          [-1, 512, 14, 14]               0\n",
      "          Conv2d-320          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-321          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-322          [-1, 512, 14, 14]               0\n",
      "          Conv2d-323         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-324         [-1, 2048, 14, 14]           4,096\n",
      "          Conv2d-325         [-1, 2048, 14, 14]       2,097,152\n",
      "     BatchNorm2d-326         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-327         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-328         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-329          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-330          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-331          [-1, 512, 14, 14]               0\n",
      "          Conv2d-332          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-333          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-334          [-1, 512, 14, 14]               0\n",
      "          Conv2d-335         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-336         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-337         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-338         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-339          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-340          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-341          [-1, 512, 14, 14]               0\n",
      "          Conv2d-342          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-343          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-344          [-1, 512, 14, 14]               0\n",
      "          Conv2d-345         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-346         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-347         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-348         [-1, 2048, 14, 14]               0\n",
      "          ResNet-349         [-1, 2048, 14, 14]               0\n",
      "AdaptiveAvgPool2d-350           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-351            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-352            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-353            [-1, 512, 1, 1]               0\n",
      "      ConvBnRelu-354            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-355           [-1, 2048, 2, 2]               0\n",
      "          Conv2d-356            [-1, 512, 2, 2]       1,048,576\n",
      "     BatchNorm2d-357            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-358            [-1, 512, 2, 2]               0\n",
      "      ConvBnRelu-359            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-360           [-1, 2048, 3, 3]               0\n",
      "          Conv2d-361            [-1, 512, 3, 3]       1,048,576\n",
      "     BatchNorm2d-362            [-1, 512, 3, 3]           1,024\n",
      "            ReLU-363            [-1, 512, 3, 3]               0\n",
      "      ConvBnRelu-364            [-1, 512, 3, 3]               0\n",
      "AdaptiveAvgPool2d-365           [-1, 2048, 6, 6]               0\n",
      "          Conv2d-366            [-1, 512, 6, 6]       1,048,576\n",
      "     BatchNorm2d-367            [-1, 512, 6, 6]           1,024\n",
      "            ReLU-368            [-1, 512, 6, 6]               0\n",
      "      ConvBnRelu-369            [-1, 512, 6, 6]               0\n",
      "          Conv2d-370          [-1, 512, 14, 14]      18,874,368\n",
      "     BatchNorm2d-371          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-372          [-1, 512, 14, 14]               0\n",
      "      ConvBnRelu-373          [-1, 512, 14, 14]               0\n",
      "       Dropout2d-374          [-1, 512, 14, 14]               0\n",
      "          Conv2d-375           [-1, 10, 14, 14]           5,130\n",
      "  PyramidPooling-376           [-1, 10, 14, 14]               0\n",
      "================================================================\n",
      "Total params: 65,702,858\n",
      "Trainable params: 65,702,858\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 535.62\n",
      "Params size (MB): 250.64\n",
      "Estimated Total Size (MB): 786.83\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 95>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m net \u001b[38;5;241m=\u001b[39m PSPNet(class_num\u001b[38;5;241m=\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     91\u001b[0m summary(net, ( \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[1;32m---> 92\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mPSPNet.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m#print(c)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mResnet101(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m psp_fm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpsp_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#print(psp_fm.size())\u001b[39;00m\n\u001b[0;32m     82\u001b[0m pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(psp_fm, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mPyramidPooling.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m ppm_out \u001b[38;5;241m=\u001b[39m [x]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pooling \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppm:\n\u001b[0;32m     62\u001b[0m     ppm_out\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m---> 63\u001b[0m         F\u001b[38;5;241m.\u001b[39minterpolate(\u001b[43mpooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, size\u001b[38;5;241m=\u001b[39m(input_size[\u001b[38;5;241m2\u001b[39m], input_size[\u001b[38;5;241m3\u001b[39m]),\n\u001b[0;32m     64\u001b[0m                       mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     65\u001b[0m ppm_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(ppm_out, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m ppm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv6(ppm_out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mConvBnRelu.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_bn:\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_relu:\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2419\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2407\u001b[0m         batch_norm,\n\u001b[0;32m   2408\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2416\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2417\u001b[0m     )\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m-> 2419\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2422\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[0;32m   2423\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2387\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   2386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 2387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[1;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import importipynb\n",
    "import resnet101\n",
    "from collections import OrderedDict\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class ConvBnRelu(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, ksize, stride, pad, dilation=1,\n",
    "                 groups=1, has_bn=True, norm_layer=nn.BatchNorm2d, bn_eps=1e-5,\n",
    "                 has_relu=True, inplace=True, has_bias=False):\n",
    "        super(ConvBnRelu, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=ksize,\n",
    "                              stride=stride, padding=pad,\n",
    "                              dilation=dilation, groups=groups, bias=has_bias)\n",
    "        self.has_bn = has_bn\n",
    "        if self.has_bn:\n",
    "            self.bn = norm_layer(out_planes, eps=bn_eps)\n",
    "        self.has_relu = has_relu\n",
    "        if self.has_relu:\n",
    "            self.relu = nn.ReLU(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.has_bn:\n",
    "            x = self.bn(x)\n",
    "        if self.has_relu:\n",
    "            x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PyramidPooling(nn.Module):\n",
    "    def __init__(self, name, out_planes, fc_dim=4096, pool_scales=[1, 2, 3, 6],\n",
    "                 norm_layer=nn.BatchNorm2d):\n",
    "        super(PyramidPooling, self).__init__()\n",
    "\n",
    "        self.ppm = []\n",
    "        for scale in pool_scales:\n",
    "            self.ppm.append(nn.Sequential(OrderedDict([\n",
    "                ('{}/pool_1'.format(name), nn.AdaptiveAvgPool2d(scale)),\n",
    "                ('{}/cbr'.format(name),\n",
    "                 ConvBnRelu(fc_dim, 512, 1, 1, 0, has_bn=True,\n",
    "                            has_relu=True, has_bias=False,\n",
    "                            norm_layer=norm_layer))\n",
    "            ])))\n",
    "        self.ppm = nn.ModuleList(self.ppm)\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            ConvBnRelu(fc_dim + len(pool_scales) * 512, 512, 3, 1, 1,\n",
    "                       has_bn=True,\n",
    "                       has_relu=True, has_bias=False, norm_layer=norm_layer),\n",
    "            nn.Dropout2d(0.1, inplace=False),\n",
    "            nn.Conv2d(512, out_planes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.size()\n",
    "        ppm_out = [x]\n",
    "        for pooling in self.ppm:\n",
    "            ppm_out.append(\n",
    "                F.interpolate(pooling(x), size=(input_size[2], input_size[3]),\n",
    "                              mode='bilinear', align_corners=True))\n",
    "        ppm_out = torch.cat(ppm_out, 1)\n",
    "\n",
    "        ppm_out = self.conv6(ppm_out)\n",
    "        return ppm_out\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, class_num, bn_momentum=0.01):\n",
    "        super(PSPNet, self).__init__()\n",
    "        self.Resnet101 = resnet101.get_resnet101(dilation=[1, 1, 1, 2], bn_momentum=bn_momentum, is_fpn=False)\n",
    "        self.psp_layer = PyramidPooling('psp', class_num, 2048, norm_layer=nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.shape\n",
    "        #print(c)\n",
    "        x = self.Resnet101(input)\n",
    "        psp_fm = self.psp_layer(x)\n",
    "        #print(psp_fm.size())\n",
    "        pred = F.interpolate(psp_fm, size=input.size()[2:4], mode='bilinear', align_corners=True)\n",
    "        #print(pred.size())\n",
    "        return pred\n",
    "\n",
    "def main():\n",
    "    num_classes = 1\n",
    "    in_batch, inchannel, in_h, in_w = 4, 3, 224, 224\n",
    "    x = torch.randn(in_batch, inchannel, in_h, in_w).to(device)\n",
    "    net = PSPNet(class_num=num_classes).to(device)\n",
    "    summary(net, ( 3, 224, 224))\n",
    "    out = net(x)\n",
    "    print(out.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc30acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
